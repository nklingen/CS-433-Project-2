{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from constants import *\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "\n",
    "#!/usr/bin/python\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Whether to display the logs during training\n",
    "DISPLAY = False\n",
    "TRAINING_SIZE = 4 # Debug purposes\n",
    "NUM_EPOCHS = 3\n",
    "N_CLASSES = 2\n",
    "NR_TEST_IMAGES = 2 # 50\n",
    "rotateFlag = False\n",
    "RATIO=0.5\n",
    "IMG_PATCH_SIZE = 16\n",
    "\n",
    "submissionFileName = \"latestSubmission.csv\"\n",
    "data_dir = '../Datasets/training/'\n",
    "test_dir = '../Datasets/test_set_images/test_'\n",
    "train_data_filename = 'images/'\n",
    "train_labels_filename = 'groundtruth/'\n",
    "RELOAD_MODEL = False\n",
    "MODEL_PATH = \"./model.pkg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# U-NET\n",
    "# Architecture based on the initial paper\n",
    "# -------------------------------------------\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 2):\n",
    "        super(UNET,self).__init__()\n",
    "        # Contracting path\n",
    "        self.contract1 = self.doubleConv_block(in_channels, 64)\n",
    "        self.contract2 = self.doubleConv_block(64, 128)\n",
    "        self.contract3 = self.doubleConv_block(128, 256)\n",
    "        self.contract4 = self.doubleConv_block(256, 512)\n",
    "\n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        # Expansive path\n",
    "        self.expand5 = self.expanding_block(512, 1024)\n",
    "        self.expand4 = self.expanding_block(1024, 512)\n",
    "        self.expand3 = self.expanding_block(512, 256)\n",
    "        self.expand2 = self.expanding_block(256, 128)\n",
    "\n",
    "        # Final block\n",
    "        self.output = self.output_block(128, out_channels)\n",
    "\n",
    "\n",
    "    def doubleConv_block(self, in_channels, out_channels):\n",
    "        \"\"\" (conv + ReLU + BN) * 2 times \"\"\"\n",
    "        doubleConv_block = torch.nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size = 3),\n",
    "        torch.nn.ReLU(),  # apply activation function\n",
    "        # Position of Batch Normalization (BN) wrt nonlinearity unclear, but experiments are generally in favor of this solution, which is the current default(ReLU + BN)\n",
    "        torch.nn.BatchNorm2d(out_channels),\n",
    "        torch.nn.Conv2d(out_channels, out_channels, kernel_size = 3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        return doubleConv_block\n",
    "\n",
    "\n",
    "\n",
    "    def expanding_block(self, in_channels, tmp_channels):\n",
    "        \"\"\" (conv + ReLU + BN) * 2 times + upconv \"\"\"\n",
    "        out_channels = tmp_channels // 2\n",
    "        expanding_block = torch.nn.Sequential(\n",
    "            self.doubleConv_block(in_channels, tmp_channels),\n",
    "            torch.nn.ConvTranspose2d(tmp_channels, out_channels, kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        return expanding_block\n",
    "\n",
    "\n",
    "    def output_block(self, in_channels = 128, out_channels = 2):\n",
    "        tmp_channels = in_channels // 2\n",
    "        output_block = torch.nn.Sequential(\n",
    "            self.doubleConv_block(in_channels, tmp_channels),\n",
    "            torch.nn.Conv2d(in_channels = tmp_channels, out_channels = out_channels, kernel_size = 1),\n",
    "        )\n",
    "        return output_block\n",
    "\n",
    "    def concatenating_block(self, x_contracting, x_expanding):\n",
    "        delta2 = (x_contracting.size()[2] - x_expanding.size()[2])\n",
    "        delta = delta2 // 2\n",
    "        if delta2 % 2 == 0 :\n",
    "        # see which type of padding to apply\n",
    "            x_cropped = F.pad(x_contracting, (-delta, -delta, -delta, -delta))\n",
    "        else :\n",
    "            x_cropped = F.pad(x_contracting, (-delta - 1, -delta, -delta - 1, -delta))\n",
    "        return torch.cat([x_cropped, x_expanding], dim = 1)\n",
    "\n",
    "\n",
    "    def forward(self, layer0):\n",
    "        # Padding with reflection\n",
    "        # pad size found such that after doing all the convolutions n_out = n_in <=> n_padded = n_in + 194 (to be verified)\n",
    "        # Can't obtain a perfect padding to obtain a 200 * 200 image in the end : pad of 93 => final image 388 * 388 / pad of 94 => 404 * 404\n",
    "        pad = 94\n",
    "        layer0 = F.pad(layer0, (pad, pad, pad, pad), mode = 'reflect')\n",
    "        print('layer0', layer0.shape)\n",
    "\n",
    "        layer1_descending = self.contract1(layer0)\n",
    "        print('layer1d', layer1_descending.shape)\n",
    "        layer2_descending = self.contract2(self.maxpool(layer1_descending))\n",
    "        print('layer2d', layer2_descending.shape)\n",
    "        layer3_descending = self.contract3(self.maxpool(layer2_descending))\n",
    "        print('layer3d', layer3_descending.shape)\n",
    "        layer4_descending = self.contract4(self.maxpool(layer3_descending))\n",
    "        print('layer4d', layer4_descending.shape)\n",
    "        layer5 = self.maxpool(layer4_descending)\n",
    "        print('layer5', layer5.shape)\n",
    "\n",
    "\n",
    "        # _ascending = input of the layer\n",
    "        layer4_ascending = self.expand5(layer5)\n",
    "        print('layer4a', layer4_ascending.shape)\n",
    "        layer3_ascending = self.expand4(self.concatenating_block(layer4_descending, layer4_ascending))\n",
    "        print('layer3a', layer3_ascending.shape)\n",
    "        layer2_ascending = self.expand3(self.concatenating_block(layer3_descending, layer3_ascending))\n",
    "        print('layer2a', layer2_ascending.shape)\n",
    "        layer1_ascending = self.expand2(self.concatenating_block(layer2_descending, layer2_ascending))\n",
    "        print('layer1a', layer1_ascending.shape)\n",
    "\n",
    "        output = self.output(self.concatenating_block(layer1_descending, layer1_ascending))\n",
    "        print('output', output.shape)\n",
    "\n",
    "        # Converting into vector\n",
    "        #output = output.view(TRAINING_SIZE, N_CLASSES, -1)\n",
    "        #print('output vector', output.shape)\n",
    "        #finally no need to convert into vector with nn.crossentropy.loss\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_UNET():\n",
    "    network = UNET()\n",
    "    network.to(DEVICE)\n",
    "    return network\n",
    "\n",
    "\n",
    "class smaller_UNET(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 2):\n",
    "        super(smaller_UNET,self).__init__()\n",
    "        # Contracting path\n",
    "        self.contract1 = self.doubleConv_block(in_channels, 32)\n",
    "        self.contract2 = self.doubleConv_block(32, 64)\n",
    "        self.contract3 = self.doubleConv_block(64, 128)\n",
    "        self.contract4 = self.doubleConv_block(128, 256)\n",
    "\n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        # Expansive path\n",
    "        self.expand5 = self.expanding_block(256, 512)\n",
    "        self.expand4 = self.expanding_block(512, 256)\n",
    "        self.expand3 = self.expanding_block(256, 128)\n",
    "        self.expand2 = self.expanding_block(128, 64)\n",
    "\n",
    "        # Final block\n",
    "        self.output = self.output_block(64, out_channels)\n",
    "\n",
    "\n",
    "    def doubleConv_block(self, in_channels, out_channels):\n",
    "        \"\"\" (conv + ReLU + BN) * 2 times \"\"\"\n",
    "        doubleConv_block = torch.nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size = 3),\n",
    "        torch.nn.ReLU(),  # apply activqtion function\n",
    "        # Position of Batch Normalization (BN) wrt nonlinearity unclear, but experiments are generally in favor of this solution, which is the current default(ReLU + BN)\n",
    "        torch.nn.BatchNorm2d(out_channels),\n",
    "        torch.nn.Conv2d(out_channels, out_channels, kernel_size = 3),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        return doubleConv_block\n",
    "\n",
    "\n",
    "\n",
    "    def expanding_block(self, in_channels, tmp_channels):\n",
    "        \"\"\" (conv + ReLU + BN) * 2 times + upconv \"\"\"\n",
    "        out_channels = tmp_channels // 2\n",
    "        expanding_block = torch.nn.Sequential(\n",
    "            self.doubleConv_block(in_channels, tmp_channels),\n",
    "            torch.nn.ConvTranspose2d(tmp_channels, out_channels, kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        return expanding_block\n",
    "\n",
    "\n",
    "    def output_block(self, in_channels = 128, out_channels = 2):\n",
    "        tmp_channels = in_channels // 2\n",
    "        output_block = torch.nn.Sequential(\n",
    "            self.doubleConv_block(in_channels, tmp_channels),\n",
    "            torch.nn.Conv2d(in_channels = tmp_channels, out_channels = out_channels, kernel_size = 1),\n",
    "        )\n",
    "        return output_block\n",
    "\n",
    "    def concatenating_block(self, x_contracting, x_expanding):\n",
    "        delta2 = (x_contracting.size()[2] - x_expanding.size()[2])\n",
    "        delta = delta2 // 2\n",
    "        if delta2 % 2 == 0 :\n",
    "        # see which type of padding to apply\n",
    "            x_cropped = F.pad(x_contracting, (-delta, -delta, -delta, -delta))\n",
    "        else :\n",
    "            x_cropped = F.pad(x_contracting, (-delta - 1, -delta, -delta - 1, -delta))\n",
    "        return torch.cat([x_cropped, x_expanding], dim = 1)\n",
    "\n",
    "\n",
    "    def forward(self, layer0):\n",
    "        # Padding with reflection\n",
    "        # pad size found such that after doing all the convolutions n_out = n_in <=> n_padded = n_in + 194 (to be verified)\n",
    "        # Can't obtain a perfect padding to obtain a 200 * 200 image in the end : pad of 93 => final image 388 * 388 / pad of 94 => 404 * 404\n",
    "        pad = 94\n",
    "        layer0 = F.pad(layer0, (pad, pad, pad, pad), mode = 'reflect')\n",
    "        print('layer0', layer0.shape)\n",
    "\n",
    "        layer1_descending = self.contract1(layer0)\n",
    "        print('layer1d', layer1_descending.shape)\n",
    "        layer2_descending = self.contract2(self.maxpool(layer1_descending))\n",
    "        print('layer2d', layer2_descending.shape)\n",
    "        layer3_descending = self.contract3(self.maxpool(layer2_descending))\n",
    "        print('layer3d', layer3_descending.shape)\n",
    "        layer4_descending = self.contract4(self.maxpool(layer3_descending))\n",
    "        print('layer4d', layer4_descending.shape)\n",
    "        layer5 = self.maxpool(layer4_descending)\n",
    "        print('layer5', layer5.shape)\n",
    "\n",
    "\n",
    "        # _ascending = input of the layer\n",
    "        layer4_ascending = self.expand5(layer5)\n",
    "        print('layer4a', layer4_ascending.shape)\n",
    "        layer3_ascending = self.expand4(self.concatenating_block(layer4_descending, layer4_ascending))\n",
    "        print('layer3a', layer3_ascending.shape)\n",
    "        layer2_ascending = self.expand3(self.concatenating_block(layer3_descending, layer3_ascending))\n",
    "        print('layer2a', layer2_ascending.shape)\n",
    "        layer1_ascending = self.expand2(self.concatenating_block(layer2_descending, layer2_ascending))\n",
    "        print('layer1a', layer1_ascending.shape)\n",
    "\n",
    "        output = self.output(self.concatenating_block(layer1_descending, layer1_ascending))\n",
    "        print('output', output.shape)\n",
    "\n",
    "        # Converting into vector\n",
    "        #output = output.view(TRAINING_SIZE, N_CLASSES, -1)\n",
    "        #print('output vector', output.shape)\n",
    "        #finally no need to convert into vector with nn.crossentropy.loss\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def create_smallerUNET():\n",
    "    network = smaller_UNET()\n",
    "    network.to(DEVICE)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen score : F1 metrics to be in accordance with AIcrowd\n",
    "\n",
    "def score(y_true,y_pred_onehot):\n",
    "\n",
    "    softMax = torch.nn.Softmax(1)\n",
    "    y_pred = torch.argmax(softMax(y_pred_onehot),1)\n",
    "\n",
    "    y_true_0 = y_true == False\n",
    "    y_pred_0 = y_pred == False\n",
    "\n",
    "    y_true_1 = y_true == True\n",
    "    y_pred_1 = y_pred == True\n",
    "\n",
    "    true_negative_nb = len(y_true[y_true_0 & y_pred_0])\n",
    "    false_negative_nb = len(y_true[y_true_1 & y_pred_0])\n",
    "    true_positive_nb = len(y_true[y_true_1 & y_pred_1])\n",
    "    false_positive_nb = len(y_true[y_true_0 & y_pred_1])\n",
    "    \n",
    "    eps = 1 # to avoid division by zero \n",
    "    # to be changed !\n",
    "\n",
    "    precision = true_positive_nb /(true_positive_nb + false_positive_nb + eps)\n",
    "    recall = true_positive_nb / (true_positive_nb + false_negative_nb + eps)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def split_data(x,y,ratio, seed = 1):\n",
    "    print(x.shape)\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_img = x.shape[0]\n",
    "    indices = np.random.permutation(num_img)\n",
    "    index_split = int(np.floor(ratio * num_img))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_val = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_val = x[index_val]\n",
    "    y_tr = y[index_tr]\n",
    "    y_val = y[index_val]\n",
    "    return x_tr, x_val, y_tr, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, loss_function, optimizer, x, y, epochs, ratio):\n",
    "    val_loss_hist = []\n",
    "    val_acc_hist = []\n",
    "    train_acc_hist = []\n",
    "    train_loss_hist = []\n",
    "\n",
    "    x,val_x,y, val_y = split_data(x, y, ratio)\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        print(\"Training, epoch=\", epoch)\n",
    "        print(\"Memory usage {0:.2f} GB\".format(process.memory_info().rss/1024/1024/1024))\n",
    "        loss_value = 0.0\n",
    "        correct = 0\n",
    "        for i in range(0,x.shape[0],BATCH_SIZE):\n",
    "            data_inputs = x[i:BATCH_SIZE+i].to(DEVICE)\n",
    "            data_targets = y[i:BATCH_SIZE+i].to(DEVICE)\n",
    "\n",
    "            print(\"Training image \", str(i))\n",
    "            print(\"Memory usage {0:.2f} GB\".format(process.memory_info().rss/1024/1024/1024))\n",
    "\n",
    "            #Traning step\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_inputs)\n",
    "            loss = loss_function(outputs, data_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #Log\n",
    "            loss_value += loss.item()\n",
    "            correct += score(data_targets,outputs)\n",
    "\n",
    "        loss_value /= x.shape[0]\n",
    "        accuracy = correct/x.shape[0]\n",
    "\n",
    "        #Validation prediction\n",
    "        \n",
    "        model.eval()\n",
    "        loss_val_value = 0.0\n",
    "        correct_val = 0\n",
    "        for i in range(0,val_x.shape[0],BATCH_SIZE):\n",
    "           \n",
    "            \n",
    "            data_val_inputs = val_x[i:BATCH_SIZE+i].to(DEVICE)\n",
    "            data_val_targets = val_y[i:BATCH_SIZE+i].to(DEVICE)\n",
    "        \n",
    "            outputs_val = model(data_val_inputs)\n",
    "            val_loss = loss_function(outputs_val,data_val_targets)\n",
    "            \n",
    "             # log \n",
    "            loss_val_value +=val_loss.item()\n",
    "            correct_val += score(data_val_targets,outputs_val)\n",
    "        \n",
    "        \n",
    "        loss_val_value /= val_x.shape[0]\n",
    "        accuracy_val = correct_val/val_x.shape[0]\n",
    "\n",
    "        #Log\n",
    "        val_loss_hist.append(loss_val_value)\n",
    "        val_acc_hist.append(accuracy_val)\n",
    "        train_loss_hist.append(loss_value)\n",
    "        train_acc_hist.append(accuracy)\n",
    "\n",
    "        if DISPLAY:\n",
    "            print(f'Epoch {epoch}, loss: {loss_value:.5f}, accuracy: {accuracy:.3f}, Val_loss: {loss_val_value:.5f}, Val_acc: {accuracy_val:.3f}')\n",
    "\n",
    "        print(\">> Saving Model for Epoch \", str(epoch))\n",
    "        checkpoint = {'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict()}\n",
    "        torch.save(checkpoint, './model')\n",
    "\n",
    "    return val_loss_hist,train_loss_hist,val_acc_hist,train_acc_hist\n",
    "\n",
    "\n",
    "#Plot the logs of the loss and accuracy on the train/validation set\n",
    "def plot_hist(val_loss_hist,train_loss_hist,val_acc_hist,train_acc_hist):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    ax1.plot(train_loss_hist,label='training')\n",
    "    ax1.plot(val_loss_hist,label='validation')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(train_acc_hist,label='training')\n",
    "    ax2.plot(val_acc_hist,label='validation')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_save_predictions(network, test_imgs):\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(test_imgs.shape)\n",
    "    softMax = torch.nn.Softmax(0)\n",
    "    filenames_list = []\n",
    "    for i in range(0, NR_TEST_IMAGES): # test_imgs.shape[0]\n",
    "        print(\"Create prediction for image #\", i+1)\n",
    "        #filename = \"/content/predictions/prediction_\" + str(i+1) + \".png\"\n",
    "        filename = \"../Datasets/Prediction/_\" + str(i+1) + \".png\"\n",
    "        filenames_list.append(filename)\n",
    "        print(\"Memory usage {0:.2f} GB\".format(process.memory_info().rss/1024/1024/1024))\n",
    "\n",
    "        image = test_imgs[i]\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        image = network(image)\n",
    "        image = image[0]\n",
    "\n",
    "        image = torch.argmax(softMax(image), 0)\n",
    "        image = image.cpu().detach().numpy()\n",
    "        image = image[2:610, 2:610]\n",
    "        print(image.shape)\n",
    "        Image.fromarray(255*image.astype('uint8')).save(filename)\n",
    "\n",
    "    return filenames_list\n",
    "\n",
    "def mean_std(x_ls):\n",
    "    x_mean = torch.mean(torch.tensor(x_ls)).item()\n",
    "    x_std = torch.std(torch.tensor(x_ls)).item()\n",
    "    return x_mean, x_std\n",
    "\n",
    "\n",
    "# Run the round times the whole process to see the variability with different initialisations (=> may not be feasible in our case)\n",
    "def round_test(create_net, x, y, epoch, score, x_test, y_test):\n",
    "    train_ls = []\n",
    "    test_ls = []\n",
    "    true_test_ls = []\n",
    "\n",
    "    for i in range(ROUNDS):\n",
    "        torch.manual_seed(2*i+1)\n",
    "        network, loss_function, optimizer = create_net()\n",
    "\n",
    "        print(\"Round {}\".format(i),end=\"\\r\")\n",
    "        val_loss_hist,train_loss_hist,val_acc_hist,train_acc_hist = training(network, loss_function, optimizer, score, x, y, epoch,val_split=0.01)\n",
    "\n",
    "        #log\n",
    "        train_ls.append(train_acc_hist[-1])\n",
    "        test_ls.append(test(network,score,x_test,y_test))\n",
    "\n",
    "        #Clear cache of GPU\n",
    "        del network\n",
    "        if torch.cuda.is_available():\n",
    "            torch._C._cuda_emptyCache()\n",
    "\n",
    "    return mean_std(train_ls),mean_std(test_ls),mean_std(true_test_ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i:i + patch_size, j:j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn in image_filenames:\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission to mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 16\n",
    "w = h\n",
    "imgwidth = int(math.ceil((600.0/w))*w)\n",
    "imgheight = int(math.ceil((600.0/h))*h)\n",
    "nc = 3\n",
    "\n",
    "# Convert an array of binary labels to a uint8\n",
    "def binary_to_uint8(img):\n",
    "    rimg = (img * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "def reconstruct_from_labels(image_id, label_file):\n",
    "    im = np.zeros((imgwidth, imgheight), dtype=np.uint8)\n",
    "    f = open(label_file)\n",
    "    lines = f.readlines()\n",
    "    image_id_str = '%.3d_' % image_id\n",
    "    for i in range(1, len(lines)):\n",
    "        line = lines[i]\n",
    "        if not image_id_str in line:\n",
    "            continue\n",
    "\n",
    "        tokens = line.split(',')\n",
    "        id = tokens[0]\n",
    "        prediction = int(tokens[1])\n",
    "        tokens = id.split('_')\n",
    "        i = int(tokens[1])\n",
    "        j = int(tokens[2])\n",
    "\n",
    "        je = min(j+w, imgwidth)\n",
    "        ie = min(i+h, imgheight)\n",
    "        if prediction == 0:\n",
    "            adata = np.zeros((w,h))\n",
    "        else:\n",
    "            adata = np.ones((w,h))\n",
    "        im[j:je, i:ie] = binary_to_uint8(adata)\n",
    "\n",
    "    #Image.fromarray(im).save('/content/predictions/prediction_patches_' + '%.3d' % image_id + '.png')\n",
    "    Image.fromarray(im).save('../Datasets/Prediction/prediction_patches_' + '%.3d' % image_id + '.png')\n",
    "\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/training/images/satImage_001.png\n",
      "../Datasets/training/images/satImage_002.png\n",
      "../Datasets/training/images/satImage_003.png\n",
      "../Datasets/training/images/satImage_004.png\n",
      "Train device cpu\n",
      "../Datasets/training/groundtruth/satImage_001.png\n",
      "../Datasets/training/groundtruth/satImage_002.png\n",
      "../Datasets/training/groundtruth/satImage_003.png\n",
      "../Datasets/training/groundtruth/satImage_004.png\n",
      "Labels device cpu\n",
      "Labels padded device cpu\n",
      "labels bin device cpu\n",
      "layer0 torch.Size([2, 3, 588, 588])\n",
      "layer1d torch.Size([2, 64, 584, 584])\n",
      "layer2d torch.Size([2, 128, 288, 288])\n",
      "layer3d torch.Size([2, 256, 140, 140])\n",
      "layer4d torch.Size([2, 512, 66, 66])\n",
      "layer5 torch.Size([2, 512, 33, 33])\n",
      "layer4a torch.Size([2, 512, 58, 58])\n",
      "layer3a torch.Size([2, 256, 108, 108])\n",
      "layer2a torch.Size([2, 128, 208, 208])\n",
      "layer1a torch.Size([2, 64, 408, 408])\n",
      "output torch.Size([2, 2, 404, 404])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 586, 586]           1,792\n",
      "              ReLU-2         [-1, 64, 586, 586]               0\n",
      "       BatchNorm2d-3         [-1, 64, 586, 586]             128\n",
      "            Conv2d-4         [-1, 64, 584, 584]          36,928\n",
      "              ReLU-5         [-1, 64, 584, 584]               0\n",
      "       BatchNorm2d-6         [-1, 64, 584, 584]             128\n",
      "         MaxPool2d-7         [-1, 64, 292, 292]               0\n",
      "            Conv2d-8        [-1, 128, 290, 290]          73,856\n",
      "              ReLU-9        [-1, 128, 290, 290]               0\n",
      "      BatchNorm2d-10        [-1, 128, 290, 290]             256\n",
      "           Conv2d-11        [-1, 128, 288, 288]         147,584\n",
      "             ReLU-12        [-1, 128, 288, 288]               0\n",
      "      BatchNorm2d-13        [-1, 128, 288, 288]             256\n",
      "        MaxPool2d-14        [-1, 128, 144, 144]               0\n",
      "           Conv2d-15        [-1, 256, 142, 142]         295,168\n",
      "             ReLU-16        [-1, 256, 142, 142]               0\n",
      "      BatchNorm2d-17        [-1, 256, 142, 142]             512\n",
      "           Conv2d-18        [-1, 256, 140, 140]         590,080\n",
      "             ReLU-19        [-1, 256, 140, 140]               0\n",
      "      BatchNorm2d-20        [-1, 256, 140, 140]             512\n",
      "        MaxPool2d-21          [-1, 256, 70, 70]               0\n",
      "           Conv2d-22          [-1, 512, 68, 68]       1,180,160\n",
      "             ReLU-23          [-1, 512, 68, 68]               0\n",
      "      BatchNorm2d-24          [-1, 512, 68, 68]           1,024\n",
      "           Conv2d-25          [-1, 512, 66, 66]       2,359,808\n",
      "             ReLU-26          [-1, 512, 66, 66]               0\n",
      "      BatchNorm2d-27          [-1, 512, 66, 66]           1,024\n",
      "        MaxPool2d-28          [-1, 512, 33, 33]               0\n",
      "           Conv2d-29         [-1, 1024, 31, 31]       4,719,616\n",
      "             ReLU-30         [-1, 1024, 31, 31]               0\n",
      "      BatchNorm2d-31         [-1, 1024, 31, 31]           2,048\n",
      "           Conv2d-32         [-1, 1024, 29, 29]       9,438,208\n",
      "             ReLU-33         [-1, 1024, 29, 29]               0\n",
      "      BatchNorm2d-34         [-1, 1024, 29, 29]           2,048\n",
      "  ConvTranspose2d-35          [-1, 512, 58, 58]       2,097,664\n",
      "           Conv2d-36          [-1, 512, 56, 56]       4,719,104\n",
      "             ReLU-37          [-1, 512, 56, 56]               0\n",
      "      BatchNorm2d-38          [-1, 512, 56, 56]           1,024\n",
      "           Conv2d-39          [-1, 512, 54, 54]       2,359,808\n",
      "             ReLU-40          [-1, 512, 54, 54]               0\n",
      "      BatchNorm2d-41          [-1, 512, 54, 54]           1,024\n",
      "  ConvTranspose2d-42        [-1, 256, 108, 108]         524,544\n",
      "           Conv2d-43        [-1, 256, 106, 106]       1,179,904\n",
      "             ReLU-44        [-1, 256, 106, 106]               0\n",
      "      BatchNorm2d-45        [-1, 256, 106, 106]             512\n",
      "           Conv2d-46        [-1, 256, 104, 104]         590,080\n",
      "             ReLU-47        [-1, 256, 104, 104]               0\n",
      "      BatchNorm2d-48        [-1, 256, 104, 104]             512\n",
      "  ConvTranspose2d-49        [-1, 128, 208, 208]         131,200\n",
      "           Conv2d-50        [-1, 128, 206, 206]         295,040\n",
      "             ReLU-51        [-1, 128, 206, 206]               0\n",
      "      BatchNorm2d-52        [-1, 128, 206, 206]             256\n",
      "           Conv2d-53        [-1, 128, 204, 204]         147,584\n",
      "             ReLU-54        [-1, 128, 204, 204]               0\n",
      "      BatchNorm2d-55        [-1, 128, 204, 204]             256\n",
      "  ConvTranspose2d-56         [-1, 64, 408, 408]          32,832\n",
      "           Conv2d-57         [-1, 64, 406, 406]          73,792\n",
      "             ReLU-58         [-1, 64, 406, 406]               0\n",
      "      BatchNorm2d-59         [-1, 64, 406, 406]             128\n",
      "           Conv2d-60         [-1, 64, 404, 404]          36,928\n",
      "             ReLU-61         [-1, 64, 404, 404]               0\n",
      "      BatchNorm2d-62         [-1, 64, 404, 404]             128\n",
      "           Conv2d-63          [-1, 2, 404, 404]             130\n",
      "================================================================\n",
      "Total params: 31,043,586\n",
      "Trainable params: 31,043,586\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 3037.04\n",
      "Params size (MB): 118.42\n",
      "Estimated Total Size (MB): 3157.29\n",
      "----------------------------------------------------------------\n",
      "torch.Size([4, 3, 400, 400])\n",
      "Training, epoch= 0\n",
      "Memory usage 0.04 GB\n",
      "Training image  0\n",
      "Memory usage 0.04 GB\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "Training image  1\n",
      "Memory usage 0.52 GB\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      ">> Saving Model for Epoch  0\n",
      "Training, epoch= 1\n",
      "Memory usage 2.99 GB\n",
      "Training image  0\n",
      "Memory usage 2.99 GB\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "Training image  1\n",
      "Memory usage 0.53 GB\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      ">> Saving Model for Epoch  1\n",
      "Training, epoch= 2\n",
      "Memory usage 3.16 GB\n",
      "Training image  0\n",
      "Memory usage 3.16 GB\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "Training image  1\n",
      "Memory usage 0.52 GB\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      "layer0 torch.Size([1, 3, 588, 588])\n",
      "layer1d torch.Size([1, 64, 584, 584])\n",
      "layer2d torch.Size([1, 128, 288, 288])\n",
      "layer3d torch.Size([1, 256, 140, 140])\n",
      "layer4d torch.Size([1, 512, 66, 66])\n",
      "layer5 torch.Size([1, 512, 33, 33])\n",
      "layer4a torch.Size([1, 512, 58, 58])\n",
      "layer3a torch.Size([1, 256, 108, 108])\n",
      "layer2a torch.Size([1, 128, 208, 208])\n",
      "layer1a torch.Size([1, 64, 408, 408])\n",
      "output torch.Size([1, 2, 404, 404])\n",
      ">> Saving Model for Epoch  2\n",
      "torch.Size([2, 3, 608, 608])\n",
      "Create prediction for image # 1\n",
      "Memory usage 0.37 GB\n",
      "layer0 torch.Size([1, 3, 796, 796])\n",
      "layer1d torch.Size([1, 64, 792, 792])\n",
      "layer2d torch.Size([1, 128, 392, 392])\n",
      "layer3d torch.Size([1, 256, 192, 192])\n",
      "layer4d torch.Size([1, 512, 92, 92])\n",
      "layer5 torch.Size([1, 512, 46, 46])\n",
      "layer4a torch.Size([1, 512, 84, 84])\n",
      "layer3a torch.Size([1, 256, 160, 160])\n",
      "layer2a torch.Size([1, 128, 312, 312])\n",
      "layer1a torch.Size([1, 64, 616, 616])\n",
      "output torch.Size([1, 2, 612, 612])\n",
      "(608, 608)\n",
      "Create prediction for image # 2\n",
      "Memory usage 0.02 GB\n",
      "layer0 torch.Size([1, 3, 796, 796])\n",
      "layer1d torch.Size([1, 64, 792, 792])\n",
      "layer2d torch.Size([1, 128, 392, 392])\n",
      "layer3d torch.Size([1, 256, 192, 192])\n",
      "layer4d torch.Size([1, 512, 92, 92])\n",
      "layer5 torch.Size([1, 512, 46, 46])\n",
      "layer4a torch.Size([1, 512, 84, 84])\n",
      "layer3a torch.Size([1, 256, 160, 160])\n",
      "layer2a torch.Size([1, 128, 312, 312])\n",
      "layer1a torch.Size([1, 64, 616, 616])\n",
      "output torch.Size([1, 2, 612, 612])\n",
      "(608, 608)\n",
      "Training loss =  [0.7216937243938446, 0.6521175503730774, 0.49039311707019806]\n",
      "Testing loss =  [0.7025502920150757, 0.7357142865657806, 1.5205811262130737]\n",
      "Training accuracy =  [0.4318928725134874, 0.5072757089541546, 0.6507136819903769]\n",
      "Testing accuracy =  [0.3488165686130589, 0.3488165686130589, 0.3488165686130589]\n"
     ]
    }
   ],
   "source": [
    "def readTrainingImages(TRAINING_SIZE, data_dir, path, rotate = False, save = False):\n",
    "    train_data_filename = data_dir + path\n",
    "    to_tensor = transforms.ToTensor() #ToTensor transforms the image to a tensor with range [0,1]\n",
    "    num_images = TRAINING_SIZE\n",
    "    imgs = []\n",
    "    r_imgs = []\n",
    "\n",
    "    for i in range(1, num_images+1):\n",
    "\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = train_data_filename + imageid + \".png\"\n",
    "        print(image_filename)\n",
    "        if os.path.isfile(image_filename):\n",
    "            img = Image.open(image_filename)\n",
    "            t_img = to_tensor(img).to(DEVICE) #3 [rgb] x 400 x 400\n",
    "            \n",
    "            imgs.append(t_img)\n",
    "\n",
    "            if (rotate == True):\n",
    "                width, height = img.size\n",
    "                # Rotate images\n",
    "\n",
    "                for degree in range(360):\n",
    "                    rotated_img = img.rotate(degree)\n",
    "                    # Crop to remove black parts\n",
    "                    left     = width / 4 * 0.58\n",
    "                    top      = height / 4 * 0.58\n",
    "                    right    = width - left\n",
    "                    bottom   = height - top\n",
    "                    rotated_img = rotated_img.crop((left, top, right, bottom))\n",
    "                    if (save == True):\n",
    "                        # optional, save new image on disk, to see the effect\n",
    "                        rotated_img.save(\"/content/Rotations/\"+path + imageid + \"_\"+str(degree)+\".png\")\n",
    "                    rt_img = to_tensor(rotated_img) # 3 [rgb] x 284 x 284\n",
    "                    r_imgs.append(rt_img)\n",
    "        else:\n",
    "            print(image_filename, \"is not a file, follow the README instruction to run the project. (check path)\", file=sys.stderr)\n",
    "            sys.exit()\n",
    "\n",
    "    if rotate:\n",
    "        r_imgs = torch.stack(r_imgs)\n",
    "    imgs = torch.stack(imgs)\n",
    "\n",
    "    return imgs, r_imgs\n",
    "\n",
    "\n",
    "def readTestImages(test_directory, num_images):\n",
    "    imgs = []\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    current_image_path = \"\"\n",
    "    for i in range(1, num_images+1):\n",
    "        current_image_path = test_directory + str(i) + \"/test_\" + str(i) + \".png\"\n",
    "        if os.path.isfile(current_image_path):\n",
    "            img = Image.open(current_image_path)\n",
    "            t_img = to_tensor(img).to(DEVICE) # 3 [rgb] x 600 x 600\n",
    "            imgs.append(t_img)\n",
    "\n",
    "        else:\n",
    "            print(current_image_path, \"is not a file, follow the README instruction to run the project. (check path)\", file=sys.stderr)\n",
    "            sys.exit()\n",
    "\n",
    "    imgs = torch.stack(imgs)\n",
    "    return imgs\n",
    "\n",
    "    # Assign a one-hot label to each pixel of a ground_truth image\n",
    "    # can be improved using scatter probably\n",
    "    # or see how it is done in the tf_aerial.py\n",
    "def value_to_class(img):\n",
    "    img = img.squeeze()\n",
    "    H = img.shape[0]\n",
    "    W = img.shape[1]\n",
    "    labels = torch.randn((H,W)).to(DEVICE)\n",
    "    foreground_threshold = 0.5\n",
    "    for h in range(H) :\n",
    "        for w in range(W) :\n",
    "            if img[h,w] > foreground_threshold:  # road\n",
    "                labels[h,w] = torch.tensor(1.0)\n",
    "            else:  # bgrd\n",
    "                labels[h,w] = torch.tensor(0.0)\n",
    "    return labels.long()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # process = psutil.Process(os.getpid()) ## in case we need to verify memory usage\n",
    "    # print(process.memory_info().rss/1024/1024)  # in Mbytes\n",
    "\n",
    "    # Reading test images\n",
    "    test_imgs = readTestImages(test_dir, NR_TEST_IMAGES)\n",
    "\n",
    "    # Reading training images\n",
    "    train_imgs, r_imgs = readTrainingImages(TRAINING_SIZE, data_dir, train_data_filename, rotateFlag) # satellite\n",
    "    print(\"Train device\", train_imgs.device)\n",
    "    labels, r_labels = readTrainingImages(TRAINING_SIZE, data_dir, train_labels_filename, rotateFlag) # labels\n",
    "    print(\"Labels device\", labels.device)\n",
    "    # Preprocessing\n",
    "    labels = F.pad(labels, (2, 2, 2, 2), mode = 'reflect') \n",
    "    print(\"Labels padded device\", labels.device)\n",
    "    labels_bin =  torch.stack([value_to_class(labels[i]) for i in range(TRAINING_SIZE)]) # decimal to binary\n",
    "    labels_bin.to(DEVICE)\n",
    "    print(\"labels bin device\", labels_bin.device)\n",
    "\n",
    "\n",
    "    # Creating the outline of the model we want\n",
    "    model = create_UNET() # 5 layers\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    # model, loss, optimizer = create_smallerUNET() # 4 layers\n",
    "    summary(model, input_size=(3,400,400)) # prints memory resources\n",
    "    \n",
    "    # training all (TRAINING_SIZE) images\n",
    "    val_loss_hist,train_loss_hist,val_acc_hist,train_acc_hist = training(model, loss, optimizer, train_imgs, labels_bin, NUM_EPOCHS, RATIO)\n",
    "    \n",
    "    # predicting on thw the test images\n",
    "    filenames_list = test_and_save_predictions(model, test_imgs)\n",
    "\n",
    "    # Create csv files\n",
    "    masks_to_submission(submissionFileName, filenames_list)\n",
    "\n",
    "    # create label images from csv\n",
    "    for i in range(1, NR_TEST_IMAGES+1):\n",
    "        reconstruct_from_labels(i, submissionFileName)\n",
    "\n",
    "    \n",
    "    return val_loss_hist,train_loss_hist,val_acc_hist,train_acc_hist\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "        val_loss_hist,train_loss_hist,val_acc_hist,train_acc_hist = main()\n",
    "        print(\"Training loss = \", train_loss_hist)\n",
    "        print(\"Testing loss = \", val_loss_hist)        \n",
    "        print(\"Training accuracy = \", train_acc_hist)\n",
    "        print(\"Testing accuracy = \", val_acc_hist)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
